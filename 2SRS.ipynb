{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import spectral\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import Data, PreProcessing\n",
    "from random import randint, uniform\n",
    "from sacred import Experiment\n",
    "from sacred.observers import FileStorageObserver\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical      \n",
    "from keras.layers import SeparableConv2D, Flatten, Dense, Dropout, Input, Concatenate, Add\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow tensorflow to utilise more memory \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=11000)]  # 11GB\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "ex_name = '2SRS'\n",
    "ex = Experiment(ex_name, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global variables\n",
    "configs = {\n",
    "    'dataset_short_name': 'IP',  # IP, PU, SA\n",
    "    'include_zero_labels': False,\n",
    "    'window_size': 25,\n",
    "    'window_size_s': 5,\n",
    "    'test_size': 0.9,\n",
    "    'n_components': 30,  # number of components for PCA\n",
    "    'normalize_samples': False,\n",
    "    'random_state': 792,  #randint(0, 1000) , IP=792, PU=477, SA=468\n",
    "    'perform_oversampling': False,\n",
    "    'apply_data_augmentation': False,\n",
    "    'da_max_samples': 1500,  # Maximum number of samples per class (for data augmentation)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data, labels, n_classes, dataset_name, rgb_bands = Data.load_data(configs['dataset_short_name'])\n",
    "print(f'Data shape is :{data.shape}')\n",
    "print(f'Labels shape is: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the samples and number of classes if needed\n",
    "if not configs['include_zero_labels']:\n",
    "    n_classes -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Band Normalization & Reduction (Spectral Dimension Reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we resize the data to 2D for band reduction (PCA or Auto-Encoder)\n",
    "original_shape = data.shape\n",
    "bands = original_shape[2]\n",
    "data = data.reshape((-1, data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Band reduction process\n",
    "bands = configs['n_components']\n",
    "minmax_scaler = MinMaxScaler()\n",
    "data = minmax_scaler.fit_transform(data)\n",
    "pca = PCA(n_components=configs['n_components'])\n",
    "data = pca.fit_transform(data)\n",
    "\n",
    "print(f'Data shape is :{data.shape}')\n",
    "print(f'Labels shape is: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data back to 3D\n",
    "data = data.reshape((original_shape[0], original_shape[1], bands))\n",
    "data.shape\n",
    "print(f'Data shape is :{data.shape}')\n",
    "print(f'Labels shape is: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Image Patches (For Spatial) and Flattened Image Patches (For Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image patches/cubes\n",
    "X_2d, y_2d = PreProcessing.create_image_cubes(data, labels, window_size=configs['window_size'], include_zero_labels=configs['include_zero_labels'])\n",
    "X_2ds, y_2ds = PreProcessing.create_image_cubes(data, labels, window_size=configs['window_size_s'], include_zero_labels=configs['include_zero_labels'])\n",
    "\n",
    "X_2ds.shape, X_2d.shape, y_2ds.shape, y_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unique class and samples for each class\n",
    "unique, counts = np.unique(y_2ds, return_counts=True)\n",
    "\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the Spectral data\n",
    "X_2ds = X_2ds.reshape((X_2ds.shape[0], X_2ds.shape[1]*X_2ds.shape[2], X_2ds.shape[3], 1))\n",
    "X_2ds.shape, X_2d.shape, y_2ds.shape, y_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 1D data and 2D data into training and testing sets\n",
    "X_2ds_train, X_2ds_test, y_train, y_test = train_test_split(X_2ds, y_2ds, test_size=configs['test_size'], random_state=configs['random_state'],\n",
    "                                                            stratify=y_2ds)\n",
    "X_2d_train, X_2d_test, y_train2, y_test2 = train_test_split(X_2d, y_2d, test_size=configs['test_size'], random_state=configs['random_state'],\n",
    "                                                            stratify=y_2d)\n",
    "\n",
    "# Check that the split is exactly the same\n",
    "print((y_train == y_train2).all())\n",
    "print((y_test == y_test2).all())\n",
    "\n",
    "# Reshape y2d\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "X_2ds_train.shape, X_2ds_test.shape, X_2d_train.shape, X_2d_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data augmentation\n",
    "class_weights = None\n",
    "if configs['apply_data_augmentation']:\n",
    "    unique, counts = np.unique(y_train.argmax(axis=1), return_counts=True)\n",
    "    print(unique)\n",
    "    print(counts)\n",
    "    max_sample = max(counts) if max(counts) > configs['da_max_samples'] else configs['da_max_samples']\n",
    "    print(f'Max no. of samples: {max_sample}')\n",
    "    for label, sample_count in zip(unique, counts):\n",
    "        required_samples = max_sample - sample_count\n",
    "        new_X_2d = np.zeros((required_samples, X_2d_train.shape[1], X_2d_train.shape[2], X_2d_train.shape[3]))\n",
    "        new_X_2ds = np.zeros((required_samples, X_2ds_train.shape[1], X_2ds_train.shape[2], X_2ds_train.shape[3]))\n",
    "        new_y = np.zeros((required_samples,))\n",
    "        for i in range(required_samples):\n",
    "            # Get the samples for the current label in the loop\n",
    "            samples_2d = X_2d_train[y_train.argmax(1) == label]\n",
    "            samples_2ds = X_2ds_train[y_train.argmax(1) == label]\n",
    "            # Select a random sample to perform data augmentation\n",
    "            sample_index = randint(0, samples_2d.shape[0]-1)\n",
    "            chosen_X_2d = samples_2d[sample_index]\n",
    "            chosen_X_2ds = samples_2ds[sample_index]\n",
    "            # Select a random data augmentation method\n",
    "            augmentation_method = randint(0, 4)\n",
    "            # Augment the data\n",
    "            if augmentation_method == 0:\n",
    "                new_X_2d[i] = np.rot90(chosen_X_2d, 1)\n",
    "            elif augmentation_method == 1:\n",
    "                new_X_2d[i] = np.rot90(chosen_X_2d, 2)\n",
    "            elif augmentation_method == 2:\n",
    "                new_X_2d[i] = np.rot90(chosen_X_2d, 3)\n",
    "            elif augmentation_method == 3:\n",
    "                new_X_2d[i] = np.flip(chosen_X_2d, 0)\n",
    "            else:\n",
    "                new_X_2d[i] = np.flip(chosen_X_2d, 1)\n",
    "            # Randomize row and assign it as new X\n",
    "            new_X_2ds[i] = np.copy(chosen_X_2ds)\n",
    "            np.random.shuffle(new_X_2ds[i])\n",
    "            new_y[i] = label\n",
    "\n",
    "        # Combine the new samples with the original ones\n",
    "        X_2d_train = np.concatenate((X_2d_train, new_X_2d))\n",
    "        X_2ds_train = np.concatenate((X_2ds_train, new_X_2ds))\n",
    "        y_train = y_train.argmax(1)\n",
    "        y_train = np.concatenate((y_train, new_y))\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "X_2d_train.shape, X_2ds_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the 2SRS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep2d_residual_block(input_layer, filters, kernel_size):\n",
    "    first_layer = SeparableConv2D(filters=filters, kernel_size=kernel_size,\n",
    "                                  activation='relu', padding='same')(input_layer)\n",
    "    x = SeparableConv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(first_layer)\n",
    "    x = SeparableConv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(x)\n",
    "    x = Add()([x, first_layer])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the Spectral 2D conv model\n",
    "# input2ds_layer = Input((configs['window_size_s']*configs['window_size_s'], bands, 1))\n",
    "# output2ds_layer = sep2d_residual_block(input2ds_layer, filters=64, kernel_size=(1, 5))\n",
    "# output2ds_layer = sep2d_residual_block(output2ds_layer, filters=128, kernel_size=(1, 5))\n",
    "# output2ds_layer = sep2d_residual_block(output2ds_layer, filters=256, kernel_size=(1, 5))\n",
    "# output2ds_layer = Flatten()(output2ds_layer)\n",
    "# output2ds_layer = Dense(units=256, activation='relu')(output2ds_layer)\n",
    "# output2ds_layer = Dropout(0.4)(output2ds_layer)\n",
    "# output2ds_layer = Dense(units=128, activation='relu')(output2ds_layer)\n",
    "# output2ds_layer = Dropout(0.4)(output2ds_layer)\n",
    "\n",
    "# # Compile the 1D conv model\n",
    "# model_2ds = Model(inputs=input2ds_layer, outputs=output2ds_layer)\n",
    "# model_2ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Building the Spatial 2D conv model\n",
    "# input2d_layer = Input((configs['window_size'], configs['window_size'], X_2d_train.shape[3]))\n",
    "# output2d_layer = sep2d_residual_block(input2d_layer, filters=64, kernel_size=(3, 3))\n",
    "# output2d_layer = sep2d_residual_block(output2d_layer, filters=128, kernel_size=(3, 3))\n",
    "# output2d_layer = sep2d_residual_block(output2d_layer, filters=256, kernel_size=(3, 3))\n",
    "# output2d_layer = Flatten()(output2d_layer)\n",
    "# output2d_layer = Dense(units=256, activation='relu')(output2d_layer)\n",
    "# output2d_layer = Dropout(0.4)(output2d_layer)\n",
    "# output2d_layer = Dense(units=128, activation='relu')(output2d_layer)\n",
    "# output2d_layer = Dropout(0.4)(output2d_layer)\n",
    "\n",
    "# # Compile the 1D conv model\n",
    "# model_2d = Model(inputs=input2d_layer, outputs=output2d_layer)\n",
    "# model_2d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the 1DConv model with the 2DConv model\n",
    "# output_layer = Concatenate()([output2ds_layer, output2d_layer])\n",
    "\n",
    "# # Add some dense layers after concatenating\n",
    "# output_layer = Dense(units=256, activation='relu')(output_layer)\n",
    "# output_layer = Dropout(0.4)(output_layer)\n",
    "# output_layer = Dense(units=128, activation='relu')(output_layer)\n",
    "# output_layer = Dropout(0.2)(output_layer)\n",
    "# output_layer = Dense(units=n_classes, activation='softmax')(output_layer)\n",
    "\n",
    "# # Finalize the model\n",
    "# model = Model(inputs=[model_2ds.input, model_2d.input], outputs=output_layer)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2SRS_model(input_tensor_2ds=None, input_tensor_2d=None, spectral_window_size=configs['window_size_s'],\n",
    "                     spatial_window_size=configs['window_size']):\n",
    "    # Spectral stream\n",
    "    if input_tensor_2ds != None:\n",
    "        input_layer_2ds = Input(tensor=input_tensor_2ds)\n",
    "    else:\n",
    "        input_layer_2ds = Input((spectral_window_size**2, bands, 1))\n",
    "    output_layer_2ds = sep2d_residual_block(input_layer_2ds, filters=64, kernel_size=(1, 5))\n",
    "    output_layer_2ds = sep2d_residual_block(output_layer_2ds, filters=128, kernel_size=(1, 5))\n",
    "    output_layer_2ds = sep2d_residual_block(output_layer_2ds, filters=256, kernel_size=(1, 5))\n",
    "    output_layer_2ds = Flatten()(output_layer_2ds)\n",
    "    output_layer_2ds = Dense(units=256, activation='relu')(output_layer_2ds)\n",
    "    output_layer_2ds = Dropout(0.4)(output_layer_2ds)\n",
    "    output_layer_2ds = Dense(units=128, activation='relu')(output_layer_2ds)\n",
    "    output_layer_2ds = Dropout(0.4)(output_layer_2ds)\n",
    "\n",
    "    # Spatial stream\n",
    "    if input_tensor_2d != None:\n",
    "        input_layer_2d = Input(tensor=input_tensor_2d)\n",
    "    else:\n",
    "        input_layer_2d = Input((spatial_window_size, spatial_window_size, bands))\n",
    "    output_layer_2d = sep2d_residual_block(input_layer_2d, filters=64, kernel_size=(3, 3))\n",
    "    output_layer_2d = sep2d_residual_block(output_layer_2d, filters=128, kernel_size=(3, 3))\n",
    "    output_layer_2d = sep2d_residual_block(output_layer_2d, filters=256, kernel_size=(3, 3))\n",
    "    output_layer_2d = Flatten()(output_layer_2d)\n",
    "    output_layer_2d = Dense(units=256, activation='relu')(output_layer_2d)\n",
    "    output_layer_2d = Dropout(0.4)(output_layer_2d)\n",
    "    output_layer_2d = Dense(units=128, activation='relu')(output_layer_2d)\n",
    "    output_layer_2d = Dropout(0.4)(output_layer_2d)\n",
    "\n",
    "    # Concatenation of the two streams\n",
    "    output_layer = Concatenate()([output_layer_2ds, output_layer_2d])\n",
    "    output_layer = Dense(units=256, activation='relu')(output_layer)\n",
    "    output_layer = Dropout(0.4)(output_layer)\n",
    "    output_layer = Dense(units=128, activation='relu')(output_layer)\n",
    "    output_layer = Dropout(0.2)(output_layer)\n",
    "    output_layer = Dense(units=n_classes, activation='softmax')(output_layer)\n",
    "\n",
    "    # Finalize the model\n",
    "    model = Model(inputs=[input_layer_2ds, input_layer_2d], outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flops():\n",
    "    import tensorflow as tf\n",
    "    import keras.backend as K\n",
    "    from keras.applications.mobilenet import MobileNet\n",
    "    from keras.models import Model\n",
    "\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        K.set_session(sess)\n",
    "\n",
    "        input_tensor_2ds = tf.compat.v1.placeholder('float32', shape=(1, configs['window_size_s'], bands, 1))\n",
    "        input_tensor_2d = tf.compat.v1.placeholder('float32', shape=(1, configs['window_size'], configs['window_size'], bands))\n",
    "        model = build_2SRS_model(input_tensor_2ds=input_tensor_2ds, input_tensor_2d=input_tensor_2d)\n",
    "\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()    \n",
    "        flops = tf.compat.v1.profiler.profile(sess.graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "\n",
    "        opts = tf.compat.v1.profiler.ProfileOptionBuilder.trainable_variables_parameter()    \n",
    "        params = tf.compat.v1.profiler.profile(sess.graph, run_meta=run_meta, cmd='op', options=opts)\n",
    "\n",
    "        print(\"{:,} --- {:,}\".format(flops.total_float_ops, params.total_parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's FLOPs\n",
    "# calculate_flops()  # Uncomment to calculate the flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_2SRS_model(spectral_window_size=configs['window_size_s']**2, spatial_window_size=configs['window_size'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# The path to save the model weights\n",
    "filename = f\"2SRS_{dataset_name}_{int(100 - configs['test_size'] * 100)}.hdf5\"\n",
    "dir_path = '../../weights'\n",
    "checkpoint_path = os.path.join(dir_path, filename)\n",
    "\n",
    "# Define a model checkpoint\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "t = time.time()\n",
    "history = model.fit(x=[X_2ds_train, X_2d_train], y=y_train, batch_size=100, epochs=100, callbacks=callbacks_list,\n",
    "                    validation_data=((X_2ds_test, X_2d_test), y_test))\n",
    "t = time.time() - t\n",
    "print(f\"Time taken for 2SRS training on {dataset_name} with {configs['test_size']} test size: {t} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best version of the model\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test data\n",
    "t = time.time()\n",
    "y_preds = model.predict([X_2ds_test, X_2d_test], verbose=1, batch_size=100)\n",
    "t = time.time() - t\n",
    "print(f\"Time taken for 2SRS training on {dataset_name} with {configs['test_size']} test size: {t} s\")\n",
    "\n",
    "# Reshape to be evaluated\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_preds = np.argmax(y_preds, axis=1)\n",
    "\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the confusion matrix\n",
    "cmatrix = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.matshow(cmatrix, cmap='tab20c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy of each class\n",
    "cmatrix.diagonal()/cmatrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Random state is set to: {configs['random_state']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the OA, AA, and Kappa on the test data\n",
    "overall_accuracy = accuracy_score(y_test, y_preds)\n",
    "avg_accuracy = balanced_accuracy_score(y_test, y_preds)\n",
    "cohen_kappa = cohen_kappa_score(y_test, y_preds)\n",
    "\n",
    "print('Overall accuracy: %0.2f' % (overall_accuracy * 100))\n",
    "print('Average accuracy: %0.2f' % (avg_accuracy * 100))\n",
    "print('Cohen\\'s Kappa: %0.2f' % (cohen_kappa * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracy and loss curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 7)\n",
    "ax1.set_title('Accuracy')\n",
    "ax2.set_title('Loss')\n",
    "\n",
    "ax1.plot(history.history['accuracy'])\n",
    "ax2.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image using the first 3 features of PCA of the dataset\n",
    "pca_bands = (0 , 1 ,2)\n",
    "spectral.imshow(data, pca_bands, figsize=(7, 7))\n",
    "spectral.save_rgb(r'C:\\Users\\Zharfan Adli\\Desktop\\output_pca_bands.png', data[:, :, [0, 1, 2]], colors=spectral.spy_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the ground truth labels map\n",
    "spectral.imshow(classes=labels, figsize=(7, 7))\n",
    "spectral.save_rgb(r'C:\\Users\\Zharfan Adli\\Desktop\\output.png', labels, colors=spectral.spy_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the whole data\n",
    "\n",
    "preds_all = model.predict([X_2ds, X_2d], verbose=1)\n",
    "preds_all = np.argmax(preds_all, axis=1)\n",
    "pred_map = np.zeros(labels.shape)\n",
    "\n",
    "preds_all.shape, pred_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the prediction map\n",
    "k = 0\n",
    "for i in range(pred_map.shape[0]):\n",
    "    for j in range(pred_map.shape[1]):\n",
    "        if labels[i][j] != 0:\n",
    "            pred_map[i][j] = preds_all[k] + 1\n",
    "            k += 1\n",
    "\n",
    "spectral.imshow(classes=pred_map, figsize=(7, 7))\n",
    "spectral.save_rgb(r'C:\\Users\\Zharfan Adli\\Desktop\\output_preds.png', pred_map, colors=spectral.spy_colors)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bc390a532b0edc36fdacde2d860d57810c283e399920b365f63830625d3ae28"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
